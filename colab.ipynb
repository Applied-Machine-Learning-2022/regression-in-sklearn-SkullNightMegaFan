{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/content/03_regression/02_regression_in_sklearn/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "copyright"
   },
   "source": [
    "#### Copyright 2020 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTcSKOCAomf-"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lsn0Av7YmMza"
   },
   "source": [
    "# Linear Regression With `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lsn0Ag7YmMza"
   },
   "source": [
    "We have learned about linear regression in theory. Now let's put our newly-acquired skills into practice! In this Colab we will create multiple linear regression models using the scikit-learn toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWTTVoS4OoQ2"
   },
   "source": [
    "# Creating a Dataset\n",
    "\n",
    "In this Colab we will explore different methods of applying linear regression to a dataset. If the dataset is small enough, the coefficients of a linear regression can be calculated via a simple equation. If the dataset is large, we need to use computing algorithms such as sampling and batching to calculate the coefficients.\n",
    "\n",
    "But before we get started, let's create some sample data to perform our regression on. The code below creates 1000 data points. The x-coordinates are called `coffee`, and the y-coordinates are called `energy`. So this regression is trying to predict a person's energy based on coffee intake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_TyEuBi_qul"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(213)\n",
    "\n",
    "# The size of the dataset that we'll be using to perform our linear regression.\n",
    "DATA_SET_SIZE = 1000\n",
    "\n",
    "# The maximum value of the x coordinate. The range of values of X will be\n",
    "# (0, X_MAX).\n",
    "X_MAX = 5\n",
    "\n",
    "# The y-intercept and slope are values that we'll be trying to predict via\n",
    "# linear regression.\n",
    "INTERCEPT = 4\n",
    "SLOPE = 3\n",
    "\n",
    "# Generate the x-coordinates (coffee intake) for our dataset.\n",
    "coffee = X_MAX * np.random.rand(DATA_SET_SIZE, 1)\n",
    "\n",
    "# Generate the y-coordinates (energy level) for our dataset using the linear\n",
    "# equation y = mx + b.\n",
    "energy = SLOPE * coffee + INTERCEPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAsf4qWOSQfP"
   },
   "source": [
    "Let's take a look at the dataset that was just generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QcnLIrlBRWS5"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(coffee, energy, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9_bV-puRYsM"
   },
   "source": [
    "The dataset does indeed have an $x$ range from 0 to the max $x$-value of 5. Notice that the $y$-intercept and slope match our seeded values.\n",
    "\n",
    "This dataset looks nothing like what we'd see in the real world, though. It would be trivial to fit a line to the data as is; the data is already a straight line. Let's add a little randomness to the data to make it more realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIidIGmLR2QG"
   },
   "outputs": [],
   "source": [
    "energy = energy + 2 * np.random.randn(DATA_SET_SIZE, 1)\n",
    "\n",
    "plt.plot(coffee, energy, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_teA7dekSCbu"
   },
   "source": [
    "That's much better! There is still a linear trend to the data, but there is much more noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pH3KFfPAdROS"
   },
   "source": [
    "# Running the Regression\n",
    "\n",
    "scikit-learn performs linear regression in its `LinearRegression` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "372l9BTOFD09"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(coffee, energy)\n",
    "lin_reg.coef_, lin_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDO1af98ehT2"
   },
   "source": [
    "Not bad! The random noise we added to the data prevented us from exactly predicting the slope and intercept, but our calculations were pretty close.\n",
    "\n",
    "We can use this slope and intercept to predict new $y$-values given some $x$-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cCBGwUFlFQf8"
   },
   "outputs": [],
   "source": [
    "coffee_ = np.array([[0.34], [1.65], [2.45], [3.78], [4.56]])\n",
    "\n",
    "energy_predict = lin_reg.predict(coffee_)\n",
    "energy_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "79D5EP8CfEZ6"
   },
   "outputs": [],
   "source": [
    "plt.plot(coffee, energy, 'b.')\n",
    "plt.plot(coffee_, energy_predict, 'r.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2k_FnFl9fTVR"
   },
   "source": [
    "And we can use two extreme $x$-values 0 and `X_MAX` to draw the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N1IdQ92Yfa64"
   },
   "outputs": [],
   "source": [
    "coffee_ = np.array([[0.0], [X_MAX]])\n",
    "energy_predict = lin_reg.predict(coffee_)\n",
    "\n",
    "plt.plot(coffee, energy, 'b.')\n",
    "plt.plot(coffee_, energy_predict, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-IbhNb9sfoXc"
   },
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "It is not always practical to run a linear regression using the entire training data set. For cases where training using the entire set is impractical, the stochastic gradient descent method can be used. In scikit-learn this is implemented via the `SGDRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1ad3bCyHFHY"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Create a new Stochastic Gradient Descent regressor.\n",
    "sgd_reg = SGDRegressor()\n",
    "\n",
    "# Fit the model.\n",
    "sgd_reg.fit(coffee, energy.ravel())\n",
    "\n",
    "# Display the slope and intercept.\n",
    "sgd_reg.coef_, sgd_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HiPCUcIog3Tv"
   },
   "source": [
    "You might notice that the slope and intercept aren't as accurate as what we were getting when processing the entire dataset. This is because the SGDRegressor is only using a subset of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6070sBV2hGwZ"
   },
   "source": [
    "Let's compare the regression lines calculated by the full linear regressor and the SGD one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0tqrFIlhO4f"
   },
   "outputs": [],
   "source": [
    "coffee_ = np.array([[0.0], [5.0]])\n",
    "\n",
    "lin_predict = lin_reg.predict(coffee_)\n",
    "sgd_predict = sgd_reg.predict(coffee_)\n",
    "\n",
    "plt.plot(coffee, energy, 'b.')\n",
    "plt.plot(coffee_, lin_predict, 'r-')\n",
    "plt.plot(coffee_, sgd_predict, 'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BM6d6zJdtS0F"
   },
   "source": [
    "It might be hard to see, but the red and green lines are *almost* the same but not quite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IYeviiuNi_Ut"
   },
   "source": [
    "### Challenge: Regressor Parameters\n",
    "\n",
    "The SGDRegressor has many parameters that can be tuned. Out of the box, our regressor didn't do that well. Let's see if we can tune some of the parameters of the regressor to get its predicted values for the slope and intercept closer to those we predicted using the entire dataset.\n",
    "\n",
    "Check out the [SGDRegressor documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) and look over the parameters available. Pay special attention to parameters related to learning rate and iterations over the data. See if you can tweak the parameters to get within some threshold `EPISLON` of the calculated values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cszel2xOhsXm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "# Initialize the dataset attributes.\n",
    "DATA_SET_SIZE = 1000\n",
    "X_MAX = 5\n",
    "INTERCEPT = 4\n",
    "SLOPE = 3\n",
    "\n",
    "# Generate the randomized dataset.\n",
    "coffee = X_MAX * np.random.rand(DATA_SET_SIZE, 1)\n",
    "energy = SLOPE * coffee + INTERCEPT + 2 * np.random.randn(DATA_SET_SIZE, 1)\n",
    "\n",
    "sgd_reg = SGDRegressor(\n",
    "    # TODO(you): Update the parameters to SGDRegressor.\n",
    "    )\n",
    "\n",
    "# Fit the model.\n",
    "sgd_reg.fit(coffee, energy.ravel())\n",
    "\n",
    "EPSILON = 0.05\n",
    "\n",
    "print(sgd_reg.coef_, sgd_reg.intercept_)\n",
    "if abs(SLOPE - sgd_reg.coef_) < EPSILON and abs(INTERCEPT - sgd_reg.intercept_) < EPSILON:\n",
    "  print(\"You win!\")\n",
    "else:\n",
    "  print(\"Try again :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0KM7txB7CZXL"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TftzlfnrS9jp"
   },
   "source": [
    "# Optional: The Normal Equation\n",
    "\n",
    "If the dataset being processed is small enough, then the slope and $y$-intercept of the regression line can be calculated in-memory exactly. The matrix normal equation can easily be written in NumPy, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6dHn3z3dC0R3"
   },
   "outputs": [],
   "source": [
    "# x is an Nx1 matrix containing our x-values. The first step in calculating the\n",
    "# normal equation is to create an Nx2 matrix where each \"row\" has the value 1\n",
    "# and the x value.\n",
    "coffee_ = np.c_[np.ones((DATA_SET_SIZE, 1)), coffee]\n",
    "\n",
    "norm = np.linalg.inv(coffee_.T.dot(coffee_)).dot(coffee_.T).dot(energy)\n",
    "\n",
    "calculated_intercept = norm[0][0]\n",
    "calculated_slope = norm[1][0]\n",
    "\n",
    "print(\"Calculated slope {} vs actual {}\".format(calculated_slope, SLOPE))\n",
    "print(\"Calculated intercept {} vs actual {}\".format(calculated_intercept,\n",
    "                                                    INTERCEPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0pdPAEisg0D"
   },
   "source": [
    "Notice that these values are the same as scikit-learn calculated above.\n",
    "\n",
    "We can now use these values to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8w9L1n4tD3Ol"
   },
   "outputs": [],
   "source": [
    "# Create a (5,1) matrix containing values to make predictions on.\n",
    "coffee_ = np.array([[0.34], [1.65], [2.45], [3.78], [4.56]])\n",
    "\n",
    "# Convert the matrix to a (5, 2) matrix with ones in the first column\n",
    "# in order to perform a dot-product against the calculated slope and\n",
    "# intercept.\n",
    "coffee_predict = np.c_[np.ones((5, 1)), coffee_]\n",
    "\n",
    "# Make the predictions.\n",
    "energy_predict = coffee_predict.dot(norm)\n",
    "\n",
    "# Plot the original data as blue dots.\n",
    "plt.plot(coffee, energy, 'b.')\n",
    "\n",
    "# Plot the predictions as red dots.\n",
    "plt.plot(coffee_, energy_predict, 'r.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r2dvyzHEYLOm"
   },
   "source": [
    "To plot the calculated line as we did for the scikit-learn regression, we can just plug in 0 and 5 (`X_MAX`) to the equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9g-KRzoYUpH"
   },
   "outputs": [],
   "source": [
    "coffee_ = np.array([[0.0], [X_MAX]])\n",
    "coffee_predict = np.c_[np.ones((2,1)), coffee_]\n",
    "energy_predict = coffee_predict.dot(norm)\n",
    "\n",
    "plt.plot(coffee, energy, 'b.')\n",
    "plt.plot(coffee_, energy_predict, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nqG8-MEc1SL"
   },
   "source": [
    "### Challenge: Pseudoinverse\n",
    "\n",
    "It turns out that the math operations used to calculate the Normal Equation are quite expensive. We'll explore other methods of performing a linear regression soon, but there is a purely mathematical optimization that has been discovered. The equation uses the **pseudoinverse** of the input matrix to predict $y$.\n",
    "\n",
    "Find the NumPy function that calculates the pseudoinverse of a matrix, and then use that function to write a more optimal method for finding the slope and intercept for a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ANUfb9xE412"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "# Initialize the dataset attributes.\n",
    "DATA_SET_SIZE = 1000\n",
    "X_MAX = 5\n",
    "INTERCEPT = 4\n",
    "SLOPE = 3\n",
    "\n",
    "# Generate the dataset.\n",
    "coffee = X_MAX * np.random.rand(DATA_SET_SIZE, 1)\n",
    "energy = SLOPE * coffee + INTERCEPT\n",
    "\n",
    "# Create the matrix.\n",
    "coffee_ = np.c_[np.ones((DATA_SET_SIZE, 1)), coffee]\n",
    "\n",
    "norm2 = [[0], [0]] # TODO(you): Update this line to perform an in-memory \n",
    "                   # calculation of a linear regression using the optimized \n",
    "                   # pseudoinverse equation in the place of the [[0], [0]]\n",
    "                   # matrix.\n",
    "\n",
    "calculated_intercept2 = norm2[0][0] \n",
    "calculated_slope2 = norm2[1][0]\n",
    "\n",
    "EPSILON = 0.00001\n",
    "\n",
    "if (abs(SLOPE - calculated_slope2) < EPSILON and\n",
    "    (abs(INTERCEPT - calculated_intercept2)) < EPSILON):\n",
    "  print(\"You win!\")\n",
    "else:\n",
    "  print(\"Try again :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i48pOk-KHW3L"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hhnHS71gkTGG"
   },
   "source": [
    "# Exercises\n",
    "\n",
    "For these exercises, we will download a CSV of life expectancies from [GapMinder](https://www.gapminder.org/data/) and create a linear regression predicting life expectancy in the United States."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TeZpmfYslA0n"
   },
   "source": [
    "## Exercise 1: Obtain the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MAp0fOaingQL"
   },
   "source": [
    "Download a CSV of life expectancy data from [GapMinder](https://www.gapminder.org/data/), upload it to this Colab, and read the data into memory using Pandas.\n",
    "\n",
    "Examine the data using describe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4k9TCn3KQpVe"
   },
   "source": [
    "### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wzylGqFydG7R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life-expectancy-by-country-and-year-gapminder.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "colab.ipynb\n",
      "kaggle.json\n",
      "life-expectancy-by-country-and-year-gapminder.zip\n",
      "life_expectancy_years.csv\n",
      "slides_regression_sklearn.pptx\n",
      "         country  1800  1801  1802  1803  1804  1805  1806  1807  1808  ...  \\\n",
      "0    Afghanistan  28.2  28.2  28.2  28.2  28.2  28.2  28.1  28.1  28.1  ...   \n",
      "1        Albania  35.4  35.4  35.4  35.4  35.4  35.4  35.4  35.4  35.4  ...   \n",
      "2        Algeria  28.8  28.8  28.8  28.8  28.8  28.8  28.8  28.8  28.8  ...   \n",
      "3        Andorra   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
      "4         Angola  27.0  27.0  27.0  27.0  27.0  27.0  27.0  27.0  27.0  ...   \n",
      "..           ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
      "182    Venezuela  32.2  32.2  32.2  32.2  32.2  32.2  32.2  32.2  32.2  ...   \n",
      "183      Vietnam  32.0  32.0  32.0  32.0  32.0  32.0  32.0  32.0  32.0  ...   \n",
      "184        Yemen  23.4  23.4  23.4  23.4  23.4  23.4  23.4  23.4  23.4  ...   \n",
      "185       Zambia  32.6  32.6  32.6  32.6  32.6  32.6  32.6  32.6  32.6  ...   \n",
      "186     Zimbabwe  33.7  33.7  33.7  33.7  33.7  33.7  33.7  33.7  33.7  ...   \n",
      "\n",
      "     2091  2092  2093  2094  2095  2096  2097  2098  2099  2100  \n",
      "0    76.5  76.6  76.7  76.9  77.0  77.1  77.3  77.4  77.5  77.7  \n",
      "1    87.4  87.5  87.6  87.7  87.8  87.9  88.0  88.1  88.2  88.3  \n",
      "2    88.3  88.4  88.5  88.6  88.7  88.8  88.9  89.0  89.1  89.2  \n",
      "3     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "4    78.7  78.9  79.0  79.1  79.3  79.4  79.5  79.7  79.8  79.9  \n",
      "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "182  86.2  86.3  86.5  86.6  86.7  86.9  87.0  87.1  87.2  87.3  \n",
      "183  84.3  84.4  84.5  84.6  84.7  84.8  84.9  85.0  85.2  85.3  \n",
      "184  77.3  77.4  77.5  77.7  77.8  77.9  78.0  78.2  78.3  78.4  \n",
      "185  76.8  77.0  77.1  77.3  77.4  77.6  77.7  77.8  78.0  78.1  \n",
      "186  74.5  74.6  74.7  74.8  75.0  75.1  75.3  75.4  75.5  75.7  \n",
      "\n",
      "[187 rows x 302 columns]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "! kaggle datasets download -d farisch/life-expectancy-by-country-and-year-gapminder\n",
    "\n",
    "data = \"life-expectancy-by-country-and-year-gapminder.zip\"\n",
    "with ZipFile(data, 'r') as zipObj:\n",
    "    #Extracting all the contents of zip into current directory. \n",
    "    zipObj.extractall()\n",
    "#Now we have a csv file\n",
    "#Let's find out what the csv is called\n",
    "! ls\n",
    "csv = \"life_expectancy_years.csv\"\n",
    "life_expectancy_df = pd.read_csv (csv)\n",
    "print(life_expectancy_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CkWhkKTgMGOG"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hm_yiqt9lkRh"
   },
   "source": [
    "## Exercise 2: Inspect the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wef2akPdniWy"
   },
   "source": [
    "Examine the data using head and/or tail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPk41fIoQ6sv"
   },
   "source": [
    "### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rO2eP1B_dyFA"
   },
   "outputs": [],
   "source": [
    "life_expectancy_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>1800</th>\n",
       "      <th>1801</th>\n",
       "      <th>1802</th>\n",
       "      <th>1803</th>\n",
       "      <th>1804</th>\n",
       "      <th>1805</th>\n",
       "      <th>1806</th>\n",
       "      <th>1807</th>\n",
       "      <th>1808</th>\n",
       "      <th>...</th>\n",
       "      <th>2091</th>\n",
       "      <th>2092</th>\n",
       "      <th>2093</th>\n",
       "      <th>2094</th>\n",
       "      <th>2095</th>\n",
       "      <th>2096</th>\n",
       "      <th>2097</th>\n",
       "      <th>2098</th>\n",
       "      <th>2099</th>\n",
       "      <th>2100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>92.6</td>\n",
       "      <td>92.7</td>\n",
       "      <td>92.8</td>\n",
       "      <td>92.9</td>\n",
       "      <td>93.0</td>\n",
       "      <td>93.1</td>\n",
       "      <td>93.3</td>\n",
       "      <td>93.4</td>\n",
       "      <td>93.5</td>\n",
       "      <td>93.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Syria</td>\n",
       "      <td>31.1</td>\n",
       "      <td>31.1</td>\n",
       "      <td>31.1</td>\n",
       "      <td>31.1</td>\n",
       "      <td>31.1</td>\n",
       "      <td>31.1</td>\n",
       "      <td>31.1</td>\n",
       "      <td>31.1</td>\n",
       "      <td>31.1</td>\n",
       "      <td>...</td>\n",
       "      <td>84.1</td>\n",
       "      <td>84.2</td>\n",
       "      <td>84.3</td>\n",
       "      <td>84.4</td>\n",
       "      <td>84.5</td>\n",
       "      <td>84.6</td>\n",
       "      <td>84.7</td>\n",
       "      <td>84.8</td>\n",
       "      <td>84.9</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Tajikistan</td>\n",
       "      <td>24.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>...</td>\n",
       "      <td>81.5</td>\n",
       "      <td>81.6</td>\n",
       "      <td>81.8</td>\n",
       "      <td>81.9</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.2</td>\n",
       "      <td>82.3</td>\n",
       "      <td>82.4</td>\n",
       "      <td>82.5</td>\n",
       "      <td>82.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Tanzania</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>...</td>\n",
       "      <td>81.3</td>\n",
       "      <td>81.4</td>\n",
       "      <td>81.6</td>\n",
       "      <td>81.7</td>\n",
       "      <td>81.9</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.2</td>\n",
       "      <td>82.3</td>\n",
       "      <td>82.5</td>\n",
       "      <td>82.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>30.4</td>\n",
       "      <td>30.4</td>\n",
       "      <td>30.4</td>\n",
       "      <td>30.4</td>\n",
       "      <td>30.4</td>\n",
       "      <td>30.4</td>\n",
       "      <td>30.4</td>\n",
       "      <td>30.4</td>\n",
       "      <td>30.4</td>\n",
       "      <td>...</td>\n",
       "      <td>89.0</td>\n",
       "      <td>89.1</td>\n",
       "      <td>89.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>89.5</td>\n",
       "      <td>89.6</td>\n",
       "      <td>89.7</td>\n",
       "      <td>89.8</td>\n",
       "      <td>89.9</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Timor-Leste</td>\n",
       "      <td>28.9</td>\n",
       "      <td>28.9</td>\n",
       "      <td>28.9</td>\n",
       "      <td>28.9</td>\n",
       "      <td>28.9</td>\n",
       "      <td>28.9</td>\n",
       "      <td>28.9</td>\n",
       "      <td>28.9</td>\n",
       "      <td>28.9</td>\n",
       "      <td>...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.1</td>\n",
       "      <td>82.3</td>\n",
       "      <td>82.4</td>\n",
       "      <td>82.5</td>\n",
       "      <td>82.7</td>\n",
       "      <td>82.8</td>\n",
       "      <td>83.0</td>\n",
       "      <td>83.1</td>\n",
       "      <td>83.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Togo</td>\n",
       "      <td>31.3</td>\n",
       "      <td>31.3</td>\n",
       "      <td>31.3</td>\n",
       "      <td>31.3</td>\n",
       "      <td>31.3</td>\n",
       "      <td>31.3</td>\n",
       "      <td>31.3</td>\n",
       "      <td>31.3</td>\n",
       "      <td>31.3</td>\n",
       "      <td>...</td>\n",
       "      <td>77.8</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.1</td>\n",
       "      <td>78.2</td>\n",
       "      <td>78.4</td>\n",
       "      <td>78.5</td>\n",
       "      <td>78.6</td>\n",
       "      <td>78.8</td>\n",
       "      <td>78.9</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Tonga</td>\n",
       "      <td>28.2</td>\n",
       "      <td>28.2</td>\n",
       "      <td>28.2</td>\n",
       "      <td>28.2</td>\n",
       "      <td>28.2</td>\n",
       "      <td>28.2</td>\n",
       "      <td>28.2</td>\n",
       "      <td>28.2</td>\n",
       "      <td>28.2</td>\n",
       "      <td>...</td>\n",
       "      <td>80.4</td>\n",
       "      <td>80.5</td>\n",
       "      <td>80.7</td>\n",
       "      <td>80.8</td>\n",
       "      <td>80.9</td>\n",
       "      <td>81.1</td>\n",
       "      <td>81.2</td>\n",
       "      <td>81.3</td>\n",
       "      <td>81.5</td>\n",
       "      <td>81.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Trinidad and Tobago</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.9</td>\n",
       "      <td>...</td>\n",
       "      <td>84.1</td>\n",
       "      <td>84.2</td>\n",
       "      <td>84.3</td>\n",
       "      <td>84.5</td>\n",
       "      <td>84.6</td>\n",
       "      <td>84.7</td>\n",
       "      <td>84.8</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.1</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Tunisia</td>\n",
       "      <td>28.8</td>\n",
       "      <td>28.8</td>\n",
       "      <td>28.8</td>\n",
       "      <td>28.8</td>\n",
       "      <td>28.8</td>\n",
       "      <td>28.8</td>\n",
       "      <td>28.8</td>\n",
       "      <td>28.8</td>\n",
       "      <td>28.8</td>\n",
       "      <td>...</td>\n",
       "      <td>89.1</td>\n",
       "      <td>89.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>89.5</td>\n",
       "      <td>89.6</td>\n",
       "      <td>89.7</td>\n",
       "      <td>89.8</td>\n",
       "      <td>89.9</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.9</td>\n",
       "      <td>91.0</td>\n",
       "      <td>91.1</td>\n",
       "      <td>91.3</td>\n",
       "      <td>91.4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>91.6</td>\n",
       "      <td>91.7</td>\n",
       "      <td>91.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Turkmenistan</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>79.5</td>\n",
       "      <td>79.7</td>\n",
       "      <td>79.8</td>\n",
       "      <td>79.9</td>\n",
       "      <td>80.1</td>\n",
       "      <td>80.2</td>\n",
       "      <td>80.4</td>\n",
       "      <td>80.5</td>\n",
       "      <td>80.6</td>\n",
       "      <td>80.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Uganda</td>\n",
       "      <td>25.3</td>\n",
       "      <td>25.3</td>\n",
       "      <td>25.3</td>\n",
       "      <td>25.3</td>\n",
       "      <td>25.3</td>\n",
       "      <td>25.3</td>\n",
       "      <td>25.3</td>\n",
       "      <td>25.3</td>\n",
       "      <td>25.3</td>\n",
       "      <td>...</td>\n",
       "      <td>80.9</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.2</td>\n",
       "      <td>81.3</td>\n",
       "      <td>81.5</td>\n",
       "      <td>81.6</td>\n",
       "      <td>81.8</td>\n",
       "      <td>81.9</td>\n",
       "      <td>82.1</td>\n",
       "      <td>82.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Ukraine</td>\n",
       "      <td>36.6</td>\n",
       "      <td>36.6</td>\n",
       "      <td>36.6</td>\n",
       "      <td>36.6</td>\n",
       "      <td>36.6</td>\n",
       "      <td>36.6</td>\n",
       "      <td>36.6</td>\n",
       "      <td>36.6</td>\n",
       "      <td>36.6</td>\n",
       "      <td>...</td>\n",
       "      <td>80.5</td>\n",
       "      <td>80.7</td>\n",
       "      <td>80.8</td>\n",
       "      <td>80.9</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.1</td>\n",
       "      <td>81.3</td>\n",
       "      <td>81.4</td>\n",
       "      <td>81.5</td>\n",
       "      <td>81.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>30.7</td>\n",
       "      <td>30.7</td>\n",
       "      <td>30.7</td>\n",
       "      <td>30.7</td>\n",
       "      <td>30.7</td>\n",
       "      <td>30.7</td>\n",
       "      <td>30.7</td>\n",
       "      <td>30.7</td>\n",
       "      <td>30.7</td>\n",
       "      <td>...</td>\n",
       "      <td>82.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>82.3</td>\n",
       "      <td>82.4</td>\n",
       "      <td>82.5</td>\n",
       "      <td>82.6</td>\n",
       "      <td>82.7</td>\n",
       "      <td>82.8</td>\n",
       "      <td>82.9</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>38.6</td>\n",
       "      <td>37.4</td>\n",
       "      <td>38.6</td>\n",
       "      <td>37.3</td>\n",
       "      <td>41.4</td>\n",
       "      <td>42.3</td>\n",
       "      <td>43.2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.4</td>\n",
       "      <td>...</td>\n",
       "      <td>89.7</td>\n",
       "      <td>89.8</td>\n",
       "      <td>89.9</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.5</td>\n",
       "      <td>90.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>United States</td>\n",
       "      <td>39.4</td>\n",
       "      <td>39.4</td>\n",
       "      <td>39.4</td>\n",
       "      <td>39.4</td>\n",
       "      <td>39.4</td>\n",
       "      <td>39.4</td>\n",
       "      <td>39.4</td>\n",
       "      <td>39.4</td>\n",
       "      <td>39.4</td>\n",
       "      <td>...</td>\n",
       "      <td>87.6</td>\n",
       "      <td>87.7</td>\n",
       "      <td>87.8</td>\n",
       "      <td>87.9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>88.1</td>\n",
       "      <td>88.2</td>\n",
       "      <td>88.3</td>\n",
       "      <td>88.4</td>\n",
       "      <td>88.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Uruguay</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.9</td>\n",
       "      <td>...</td>\n",
       "      <td>86.6</td>\n",
       "      <td>86.7</td>\n",
       "      <td>86.8</td>\n",
       "      <td>86.9</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.1</td>\n",
       "      <td>87.2</td>\n",
       "      <td>87.3</td>\n",
       "      <td>87.4</td>\n",
       "      <td>87.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Uzbekistan</td>\n",
       "      <td>26.9</td>\n",
       "      <td>26.9</td>\n",
       "      <td>26.9</td>\n",
       "      <td>26.9</td>\n",
       "      <td>26.9</td>\n",
       "      <td>26.9</td>\n",
       "      <td>26.9</td>\n",
       "      <td>26.9</td>\n",
       "      <td>26.9</td>\n",
       "      <td>...</td>\n",
       "      <td>78.9</td>\n",
       "      <td>79.0</td>\n",
       "      <td>79.1</td>\n",
       "      <td>79.3</td>\n",
       "      <td>79.4</td>\n",
       "      <td>79.5</td>\n",
       "      <td>79.6</td>\n",
       "      <td>79.8</td>\n",
       "      <td>79.9</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>24.3</td>\n",
       "      <td>24.3</td>\n",
       "      <td>24.3</td>\n",
       "      <td>24.3</td>\n",
       "      <td>24.3</td>\n",
       "      <td>24.3</td>\n",
       "      <td>24.3</td>\n",
       "      <td>24.3</td>\n",
       "      <td>24.3</td>\n",
       "      <td>...</td>\n",
       "      <td>73.5</td>\n",
       "      <td>73.6</td>\n",
       "      <td>73.7</td>\n",
       "      <td>73.8</td>\n",
       "      <td>73.9</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.2</td>\n",
       "      <td>74.3</td>\n",
       "      <td>74.4</td>\n",
       "      <td>74.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Venezuela</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>...</td>\n",
       "      <td>86.2</td>\n",
       "      <td>86.3</td>\n",
       "      <td>86.5</td>\n",
       "      <td>86.6</td>\n",
       "      <td>86.7</td>\n",
       "      <td>86.9</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.1</td>\n",
       "      <td>87.2</td>\n",
       "      <td>87.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Vietnam</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>84.3</td>\n",
       "      <td>84.4</td>\n",
       "      <td>84.5</td>\n",
       "      <td>84.6</td>\n",
       "      <td>84.7</td>\n",
       "      <td>84.8</td>\n",
       "      <td>84.9</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.2</td>\n",
       "      <td>85.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Yemen</td>\n",
       "      <td>23.4</td>\n",
       "      <td>23.4</td>\n",
       "      <td>23.4</td>\n",
       "      <td>23.4</td>\n",
       "      <td>23.4</td>\n",
       "      <td>23.4</td>\n",
       "      <td>23.4</td>\n",
       "      <td>23.4</td>\n",
       "      <td>23.4</td>\n",
       "      <td>...</td>\n",
       "      <td>77.3</td>\n",
       "      <td>77.4</td>\n",
       "      <td>77.5</td>\n",
       "      <td>77.7</td>\n",
       "      <td>77.8</td>\n",
       "      <td>77.9</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.2</td>\n",
       "      <td>78.3</td>\n",
       "      <td>78.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Zambia</td>\n",
       "      <td>32.6</td>\n",
       "      <td>32.6</td>\n",
       "      <td>32.6</td>\n",
       "      <td>32.6</td>\n",
       "      <td>32.6</td>\n",
       "      <td>32.6</td>\n",
       "      <td>32.6</td>\n",
       "      <td>32.6</td>\n",
       "      <td>32.6</td>\n",
       "      <td>...</td>\n",
       "      <td>76.8</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.1</td>\n",
       "      <td>77.3</td>\n",
       "      <td>77.4</td>\n",
       "      <td>77.6</td>\n",
       "      <td>77.7</td>\n",
       "      <td>77.8</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>33.7</td>\n",
       "      <td>33.7</td>\n",
       "      <td>33.7</td>\n",
       "      <td>33.7</td>\n",
       "      <td>33.7</td>\n",
       "      <td>33.7</td>\n",
       "      <td>33.7</td>\n",
       "      <td>33.7</td>\n",
       "      <td>33.7</td>\n",
       "      <td>...</td>\n",
       "      <td>74.5</td>\n",
       "      <td>74.6</td>\n",
       "      <td>74.7</td>\n",
       "      <td>74.8</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.1</td>\n",
       "      <td>75.3</td>\n",
       "      <td>75.4</td>\n",
       "      <td>75.5</td>\n",
       "      <td>75.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows × 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  country  1800  1801  1802  1803  1804  1805  1806  1807  \\\n",
       "162           Switzerland  38.0  38.0  38.0  38.0  38.0  38.0  38.0  38.0   \n",
       "163                 Syria  31.1  31.1  31.1  31.1  31.1  31.1  31.1  31.1   \n",
       "164            Tajikistan  24.2  24.2  24.2  24.2  24.2  24.2  24.2  24.2   \n",
       "165              Tanzania  32.2  32.2  32.2  32.2  32.2  32.2  32.2  32.2   \n",
       "166              Thailand  30.4  30.4  30.4  30.4  30.4  30.4  30.4  30.4   \n",
       "167           Timor-Leste  28.9  28.9  28.9  28.9  28.9  28.9  28.9  28.9   \n",
       "168                  Togo  31.3  31.3  31.3  31.3  31.3  31.3  31.3  31.3   \n",
       "169                 Tonga  28.2  28.2  28.2  28.2  28.2  28.2  28.2  28.2   \n",
       "170   Trinidad and Tobago  32.9  32.9  32.9  32.9  32.9  32.9  32.9  32.9   \n",
       "171               Tunisia  28.8  28.8  28.8  28.8  28.8  28.8  28.8  28.8   \n",
       "172                Turkey  35.0  35.0  35.0  35.0  35.0  35.0  35.0  35.0   \n",
       "173          Turkmenistan  24.0  24.0  24.0  24.0  24.0  24.0  24.0  24.0   \n",
       "174                Uganda  25.3  25.3  25.3  25.3  25.3  25.3  25.3  25.3   \n",
       "175               Ukraine  36.6  36.6  36.6  36.6  36.6  36.6  36.6  36.6   \n",
       "176  United Arab Emirates  30.7  30.7  30.7  30.7  30.7  30.7  30.7  30.7   \n",
       "177        United Kingdom  38.6  37.4  38.6  37.3  41.4  42.3  43.2  40.0   \n",
       "178         United States  39.4  39.4  39.4  39.4  39.4  39.4  39.4  39.4   \n",
       "179               Uruguay  32.9  32.9  32.9  32.9  32.9  32.9  32.9  32.9   \n",
       "180            Uzbekistan  26.9  26.9  26.9  26.9  26.9  26.9  26.9  26.9   \n",
       "181               Vanuatu  24.3  24.3  24.3  24.3  24.3  24.3  24.3  24.3   \n",
       "182             Venezuela  32.2  32.2  32.2  32.2  32.2  32.2  32.2  32.2   \n",
       "183               Vietnam  32.0  32.0  32.0  32.0  32.0  32.0  32.0  32.0   \n",
       "184                 Yemen  23.4  23.4  23.4  23.4  23.4  23.4  23.4  23.4   \n",
       "185                Zambia  32.6  32.6  32.6  32.6  32.6  32.6  32.6  32.6   \n",
       "186              Zimbabwe  33.7  33.7  33.7  33.7  33.7  33.7  33.7  33.7   \n",
       "\n",
       "     1808  ...  2091  2092  2093  2094  2095  2096  2097  2098  2099  2100  \n",
       "162  38.0  ...  92.6  92.7  92.8  92.9  93.0  93.1  93.3  93.4  93.5  93.6  \n",
       "163  31.1  ...  84.1  84.2  84.3  84.4  84.5  84.6  84.7  84.8  84.9  85.0  \n",
       "164  24.2  ...  81.5  81.6  81.8  81.9  82.0  82.2  82.3  82.4  82.5  82.7  \n",
       "165  32.2  ...  81.3  81.4  81.6  81.7  81.9  82.0  82.2  82.3  82.5  82.7  \n",
       "166  30.4  ...  89.0  89.1  89.2  89.3  89.5  89.6  89.7  89.8  89.9  90.0  \n",
       "167  28.9  ...  82.0  82.1  82.3  82.4  82.5  82.7  82.8  83.0  83.1  83.2  \n",
       "168  31.3  ...  77.8  78.0  78.1  78.2  78.4  78.5  78.6  78.8  78.9  79.0  \n",
       "169  28.2  ...  80.4  80.5  80.7  80.8  80.9  81.1  81.2  81.3  81.5  81.6  \n",
       "170  32.9  ...  84.1  84.2  84.3  84.5  84.6  84.7  84.8  85.0  85.1  85.2  \n",
       "171  28.8  ...  89.1  89.2  89.3  89.5  89.6  89.7  89.8  89.9  90.0  90.1  \n",
       "172  35.0  ...  90.8  90.9  91.0  91.1  91.3  91.4  91.5  91.6  91.7  91.8  \n",
       "173  24.0  ...  79.5  79.7  79.8  79.9  80.1  80.2  80.4  80.5  80.6  80.8  \n",
       "174  25.3  ...  80.9  81.0  81.2  81.3  81.5  81.6  81.8  81.9  82.1  82.2  \n",
       "175  36.6  ...  80.5  80.7  80.8  80.9  81.0  81.1  81.3  81.4  81.5  81.6  \n",
       "176  30.7  ...  82.1  82.2  82.3  82.4  82.5  82.6  82.7  82.8  82.9  83.0  \n",
       "177  40.4  ...  89.7  89.8  89.9  90.0  90.1  90.2  90.3  90.4  90.5  90.6  \n",
       "178  39.4  ...  87.6  87.7  87.8  87.9  88.0  88.1  88.2  88.3  88.4  88.5  \n",
       "179  32.9  ...  86.6  86.7  86.8  86.9  87.0  87.1  87.2  87.3  87.4  87.5  \n",
       "180  26.9  ...  78.9  79.0  79.1  79.3  79.4  79.5  79.6  79.8  79.9  80.0  \n",
       "181  24.3  ...  73.5  73.6  73.7  73.8  73.9  74.0  74.2  74.3  74.4  74.5  \n",
       "182  32.2  ...  86.2  86.3  86.5  86.6  86.7  86.9  87.0  87.1  87.2  87.3  \n",
       "183  32.0  ...  84.3  84.4  84.5  84.6  84.7  84.8  84.9  85.0  85.2  85.3  \n",
       "184  23.4  ...  77.3  77.4  77.5  77.7  77.8  77.9  78.0  78.2  78.3  78.4  \n",
       "185  32.6  ...  76.8  77.0  77.1  77.3  77.4  77.6  77.7  77.8  78.0  78.1  \n",
       "186  33.7  ...  74.5  74.6  74.7  74.8  75.0  75.1  75.3  75.4  75.5  75.7  \n",
       "\n",
       "[25 rows x 302 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "life_expectancy_df.tail(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_KGjg5TMOdF"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HKbB0goxl-gQ"
   },
   "source": [
    "## Exercise 3: Preprocess Life Expectancy Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DEYyHO4knkg8"
   },
   "source": [
    "Extract the life expectancy values for the United States into a NumPy array. \n",
    "\n",
    "To do this you'll need to find the row that contains data for the United States. When you find that row of data, you'll find the word 'United States' in the first column and then floating point numbers in subsequent columns. The goal of this step is to create a NumPy array containing those numbers, but excluding the first column with the title 'United States'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSyjgiQXRHYC"
   },
   "source": [
    "### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Z1Dz7Ffd6_Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 38.0,\n",
       " 34.5,\n",
       " 34.0,\n",
       " 31.0,\n",
       " 38.0,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 39.4,\n",
       " 40.0,\n",
       " 40.6,\n",
       " 41.2,\n",
       " 41.7,\n",
       " 42.3,\n",
       " 42.9,\n",
       " 43.5,\n",
       " 44.1,\n",
       " 44.6,\n",
       " 45.2,\n",
       " 45.6,\n",
       " 46.0,\n",
       " 46.3,\n",
       " 46.7,\n",
       " 47.1,\n",
       " 47.5,\n",
       " 47.8,\n",
       " 48.2,\n",
       " 48.6,\n",
       " 49.0,\n",
       " 49.3,\n",
       " 50.5,\n",
       " 50.6,\n",
       " 49.6,\n",
       " 50.3,\n",
       " 50.1,\n",
       " 50.2,\n",
       " 51.9,\n",
       " 52.8,\n",
       " 51.8,\n",
       " 53.4,\n",
       " 54.1,\n",
       " 53.5,\n",
       " 54.6,\n",
       " 55.1,\n",
       " 54.2,\n",
       " 54.0,\n",
       " 47.2,\n",
       " 55.3,\n",
       " 55.4,\n",
       " 58.3,\n",
       " 58.1,\n",
       " 57.5,\n",
       " 58.5,\n",
       " 58.5,\n",
       " 58.0,\n",
       " 59.5,\n",
       " 58.4,\n",
       " 58.5,\n",
       " 59.7,\n",
       " 60.4,\n",
       " 61.1,\n",
       " 61.0,\n",
       " 60.4,\n",
       " 61.0,\n",
       " 60.5,\n",
       " 61.2,\n",
       " 62.5,\n",
       " 63.2,\n",
       " 63.4,\n",
       " 63.9,\n",
       " 64.7,\n",
       " 64.4,\n",
       " 65.2,\n",
       " 65.7,\n",
       " 66.4,\n",
       " 66.8,\n",
       " 67.4,\n",
       " 67.8,\n",
       " 68.2,\n",
       " 68.3,\n",
       " 68.5,\n",
       " 68.9,\n",
       " 69.7,\n",
       " 69.7,\n",
       " 69.8,\n",
       " 69.6,\n",
       " 69.8,\n",
       " 70.1,\n",
       " 70.0,\n",
       " 70.4,\n",
       " 70.3,\n",
       " 70.1,\n",
       " 70.4,\n",
       " 70.5,\n",
       " 70.5,\n",
       " 70.9,\n",
       " 70.5,\n",
       " 70.8,\n",
       " 71.0,\n",
       " 71.3,\n",
       " 71.5,\n",
       " 71.8,\n",
       " 72.2,\n",
       " 72.6,\n",
       " 73.0,\n",
       " 73.3,\n",
       " 73.6,\n",
       " 73.9,\n",
       " 74.1,\n",
       " 74.4,\n",
       " 74.6,\n",
       " 74.7,\n",
       " 74.8,\n",
       " 74.9,\n",
       " 75.0,\n",
       " 75.1,\n",
       " 75.2,\n",
       " 75.3,\n",
       " 75.6,\n",
       " 75.7,\n",
       " 75.9,\n",
       " 75.8,\n",
       " 75.8,\n",
       " 75.9,\n",
       " 76.3,\n",
       " 76.6,\n",
       " 76.8,\n",
       " 76.8,\n",
       " 76.9,\n",
       " 76.9,\n",
       " 77.0,\n",
       " 77.2,\n",
       " 77.5,\n",
       " 77.6,\n",
       " 77.8,\n",
       " 78.0,\n",
       " 78.2,\n",
       " 78.4,\n",
       " 78.7,\n",
       " 78.8,\n",
       " 78.9,\n",
       " 78.9,\n",
       " 78.9,\n",
       " 78.8,\n",
       " 78.6,\n",
       " 78.6,\n",
       " 78.6,\n",
       " 78.6,\n",
       " 78.6,\n",
       " 78.7,\n",
       " 78.8,\n",
       " 78.9,\n",
       " 79.0,\n",
       " 79.1,\n",
       " 79.3,\n",
       " 79.5,\n",
       " 79.7,\n",
       " 79.9,\n",
       " 80.1,\n",
       " 80.3,\n",
       " 80.5,\n",
       " 80.8,\n",
       " 80.9,\n",
       " 81.1,\n",
       " 81.2,\n",
       " 81.4,\n",
       " 81.5,\n",
       " 81.6,\n",
       " 81.8,\n",
       " 81.9,\n",
       " 82.1,\n",
       " 82.2,\n",
       " 82.3,\n",
       " 82.5,\n",
       " 82.6,\n",
       " 82.8,\n",
       " 82.9,\n",
       " 83.0,\n",
       " 83.2,\n",
       " 83.3,\n",
       " 83.4,\n",
       " 83.5,\n",
       " 83.7,\n",
       " 83.8,\n",
       " 83.9,\n",
       " 84.0,\n",
       " 84.1,\n",
       " 84.3,\n",
       " 84.4,\n",
       " 84.5,\n",
       " 84.6,\n",
       " 84.7,\n",
       " 84.8,\n",
       " 84.9,\n",
       " 85.0,\n",
       " 85.2,\n",
       " 85.3,\n",
       " 85.4,\n",
       " 85.5,\n",
       " 85.6,\n",
       " 85.7,\n",
       " 85.8,\n",
       " 85.9,\n",
       " 86.0,\n",
       " 86.1,\n",
       " 86.2,\n",
       " 86.3,\n",
       " 86.4,\n",
       " 86.5,\n",
       " 86.6,\n",
       " 86.7,\n",
       " 86.8,\n",
       " 86.9,\n",
       " 87.0,\n",
       " 87.1,\n",
       " 87.2,\n",
       " 87.3,\n",
       " 87.4,\n",
       " 87.5,\n",
       " 87.6,\n",
       " 87.7,\n",
       " 87.8,\n",
       " 87.9,\n",
       " 88.0,\n",
       " 88.1,\n",
       " 88.2,\n",
       " 88.3,\n",
       " 88.4,\n",
       " 88.5]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "america = life_expectancy_df.iloc[178].to_numpy()\n",
    "\n",
    "america_float = []\n",
    "for element in america:\n",
    "    if type(element) == str:\n",
    "        pass\n",
    "    else:\n",
    "       america_float.append(element)\n",
    "       \n",
    "    \n",
    "america_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-e7VTXN_MRtP"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VKjiMJUYmPxB"
   },
   "source": [
    "## Exercise 4: Create Yearly Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fxaHozJBnnEZ"
   },
   "source": [
    "Now we need to create an array of year data from the minimum to the maximum year in the dataset. There are a few ways that this can be done.\n",
    "\n",
    "The column names (except column 0) are the years. You can extract those years into an array, similarly to what you did for life expectancy. Note that the column names are strings, so you'll want to convert them into integers.\n",
    "\n",
    "If no years are missing from the dataset, you can also just use a range function to generate numbers between the min and max years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QImI7PELRSmz"
   },
   "source": [
    "### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dpPEWZK-fVl7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = list(map(lambda x: int(x),life_expectancy_df.columns[1:].tolist()))\n",
    "len(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHJOCRU3NgRi"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XaAeDllAmeQk"
   },
   "source": [
    "## Exercise 5: Plot the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Av4o6raqnpeT"
   },
   "source": [
    "Create a scatterplot of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R7sBzMhmRejB"
   },
   "source": [
    "### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iY2K1x7Wf-yr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#graph = plt.scatter(years,us_exp.columns[1:].tolist())\n",
    "#us_val = us_exp.values[0][1:]\n",
    "plt.scatter(years,america_float)\n",
    "\n",
    "print(type(years))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Su5luLDDNiPE"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQp6L7K2mj6F"
   },
   "source": [
    "## Exercise 6: Subset the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygr_3N79nsQu"
   },
   "source": [
    "Split off 20% of the data as a test set, and keep the rest for training.\n",
    "\n",
    "To do this it will be useful to create a `DataFrame` and store the years and life expectancy arrays created above as columns in that dataframe.\n",
    "\n",
    "You can then randomize and split the dataframe, or use scikit-learn's built in test/train data splitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPUqZXnxR5d8"
   },
   "source": [
    "### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4D8uwmhmqqp"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>years</th>\n",
       "      <th>America</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>2006</td>\n",
       "      <td>77.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1871</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>2098</td>\n",
       "      <td>88.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>2044</td>\n",
       "      <td>82.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1855</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1876</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>2030</td>\n",
       "      <td>80.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>2081</td>\n",
       "      <td>86.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1913</td>\n",
       "      <td>53.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1805</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1824</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>2093</td>\n",
       "      <td>87.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1993</td>\n",
       "      <td>75.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1983</td>\n",
       "      <td>74.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1995</td>\n",
       "      <td>75.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1872</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2007</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1867</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>2021</td>\n",
       "      <td>78.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1904</td>\n",
       "      <td>49.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1912</td>\n",
       "      <td>54.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1873</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>2066</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1844</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1929</td>\n",
       "      <td>58.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1840</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>2097</td>\n",
       "      <td>88.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1892</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>2001</td>\n",
       "      <td>76.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2000</td>\n",
       "      <td>76.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1835</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2047</td>\n",
       "      <td>82.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1992</td>\n",
       "      <td>75.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>2012</td>\n",
       "      <td>78.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1934</td>\n",
       "      <td>60.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>2079</td>\n",
       "      <td>86.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1862</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>2063</td>\n",
       "      <td>84.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1961</td>\n",
       "      <td>70.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>2061</td>\n",
       "      <td>84.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1852</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2036</td>\n",
       "      <td>81.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2045</td>\n",
       "      <td>82.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1863</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1931</td>\n",
       "      <td>60.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1946</td>\n",
       "      <td>66.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2034</td>\n",
       "      <td>80.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1919</td>\n",
       "      <td>55.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1813</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2027</td>\n",
       "      <td>79.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>2088</td>\n",
       "      <td>87.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>2055</td>\n",
       "      <td>83.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1815</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1917</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1997</td>\n",
       "      <td>76.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1903</td>\n",
       "      <td>50.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1948</td>\n",
       "      <td>67.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1907</td>\n",
       "      <td>50.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1850</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1944</td>\n",
       "      <td>65.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     years  America\n",
       "206   2006     77.8\n",
       "71    1871     39.4\n",
       "298   2098     88.3\n",
       "244   2044     82.3\n",
       "55    1855     39.4\n",
       "76    1876     39.4\n",
       "230   2030     80.1\n",
       "281   2081     86.6\n",
       "113   1913     53.5\n",
       "5     1805     39.4\n",
       "24    1824     39.4\n",
       "293   2093     87.8\n",
       "193   1993     75.8\n",
       "183   1983     74.7\n",
       "195   1995     75.9\n",
       "72    1872     39.4\n",
       "207   2007     78.0\n",
       "67    1867     39.4\n",
       "221   2021     78.7\n",
       "104   1904     49.6\n",
       "112   1912     54.1\n",
       "73    1873     39.4\n",
       "266   2066     85.0\n",
       "44    1844     39.4\n",
       "129   1929     58.5\n",
       "40    1840     39.4\n",
       "297   2097     88.2\n",
       "92    1892     46.0\n",
       "201   2001     76.9\n",
       "200   2000     76.9\n",
       "35    1835     39.4\n",
       "247   2047     82.8\n",
       "192   1992     75.9\n",
       "212   2012     78.9\n",
       "134   1934     60.4\n",
       "279   2079     86.4\n",
       "62    1862     34.5\n",
       "263   2063     84.7\n",
       "161   1961     70.4\n",
       "261   2061     84.5\n",
       "52    1852     39.4\n",
       "236   2036     81.2\n",
       "245   2045     82.5\n",
       "63    1863     34.0\n",
       "131   1931     60.4\n",
       "146   1946     66.4\n",
       "234   2034     80.9\n",
       "119   1919     55.3\n",
       "13    1813     39.4\n",
       "227   2027     79.5\n",
       "288   2088     87.3\n",
       "255   2055     83.8\n",
       "15    1815     39.4\n",
       "117   1917     54.0\n",
       "197   1997     76.6\n",
       "103   1903     50.6\n",
       "148   1948     67.4\n",
       "107   1907     50.2\n",
       "50    1850     39.4\n",
       "144   1944     65.2"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'years':years,'America':america_float})\n",
    "frac_df = df.sample(frac=0.20)\n",
    "\n",
    "frac_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFIHbMESNsaw"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JN_s6eb_mswQ"
   },
   "source": [
    "## Exercise 7: Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pV7ZhE6qnumn"
   },
   "source": [
    "Use `LinearRegression` in scikit-learn to create a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDKFaGk4SHMA"
   },
   "source": [
    "### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-H0VAClvOACh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.21199365]]), array([-351.22151056]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(frac_df[['years']].values, frac_df[['America']].values)\n",
    "lin_reg.coef_, lin_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CbfzTfazOBq5"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dtFpVNwEnX9Q"
   },
   "source": [
    "## Exercise 8: Test Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mALMCq0SnwoR"
   },
   "source": [
    "Use the test data you put aside to make predictions of life expectancy based on year. Compare the predictions to the actual data by using scikit-learn to calculate the root mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8my8vKLBSZPE"
   },
   "source": [
    "### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xABv08xnzNg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.75500422714288"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "predict_df = lin_reg.predict(frac_df['years'].values.reshape(-1,1))\n",
    "\n",
    "mean_squared_error(frac_df['America'].values.reshape(-1,1), predict_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zgvlJJtPRWg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yN34XjZpn8YS"
   },
   "source": [
    "## Exercise 9: Plot Your Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6XxLAdoan1J6"
   },
   "source": [
    "Create a scatter plot of the full set of life expectancy data. Draw your regression line over the scatterplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CtivIm4GSoVI"
   },
   "source": [
    "### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XJUdILlKoQ4T"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f90399409d0>]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(frac_df['years'],frac_df['America'])\n",
    "plt.plot(frac_df['years'],predict_df, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A2FLyo2MPtrp"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WClC4PzQIXXr"
   },
   "source": [
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJvVUYPmn4i9"
   },
   "source": [
    "The [CalCOFI dataset](https://www.kaggle.com/sohier/calcofi/version/2) contains decades of oceanic data. In this exercise, we will use this data to attempt to predict water temperature based on salinity. The exercise is divided into multiple steps, each with a code block after it for your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ly-Yql3A7ooA"
   },
   "source": [
    "### Student Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXJfxu8lKNsg"
   },
   "source": [
    "**Acquire the data**\n",
    "\n",
    "The [CalCOFI data](https://www.kaggle.com/sohier/calcofi/version/2) consists of two files, one containing data about *Casts* and the other about *Bottles*. Look at the data files and try to get an understanding of what a cast is and what a bottle is.\n",
    "\n",
    "Find the file that contains temperature and salinity information, download that file, and then upload it to Colab. You'll want to use the zipped version of the file, so that the upload doesn't take too long.\n",
    "\n",
    "Once the file is uploaded, use Python to unzip the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lcLke7YdP4rf"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile ('bottle.csv.zip','r') as zip_module:\n",
    "  zip_module.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NAaW2FpLEe7"
   },
   "source": [
    "**Load the data using Pandas**\n",
    "\n",
    "Now that you have an unzipped version of the file, you can load the data into memory using Pandas. Write code to read the file into memory and describe the data table that you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dTXmyrbBP-dT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "soda = pd.read_csv(\"bottle.csv\")\n",
    "soda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y8My0tHlLhZV"
   },
   "source": [
    "**Drop rows with missing data**\n",
    "\n",
    "Looking at the counts for temperature and salinity, you can see that there are some rows with missing data. Remove the rows with missing temperature or salinity data from the dataframe. After you are done, describe the data to make sure that every temperature and salinity row contains data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2FSUUvtuP-4z"
   },
   "outputs": [],
   "source": [
    "soda_data = soda[soda['T_degC'].isna()==False]\n",
    "soda_data = soda[soda['Salnty'].isna()==False]\n",
    "soda_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n1pqkRynMBHK"
   },
   "source": [
    "**Plot the data**\n",
    "\n",
    "Create a scatterplot of salinity and temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Skr58bgBP_cE"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(soda_data['Salnty'],soda_data['T_degC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44I2gHTnMPbh"
   },
   "source": [
    "**Shuffle the data**\n",
    "\n",
    "In this exercise, we will split the data into a training set and a test set. Since the data is ordered, we need to shuffle the dataframe before splitting it. Write code to shuffle the dataframe, and look at the data (using `head`, `tail`, or some other means) to make sure that it is shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "77VQfHxhP__A"
   },
   "outputs": [],
   "source": [
    "soda_data = soda_data.sample(frac=1)\n",
    "soda_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WlqxPwqMrsU"
   },
   "source": [
    "**Split the data into train/test**\n",
    "\n",
    "For this exercise we'll split the data frame so that 20% of the data is held out for testing, and the remaining data is used for training. Write code to split the data into two dataframes: one for testing and one for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vGWPiYb0QAZU"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'years':years,'us_val':us_val})\n",
    "frac_df = df.sample(frac=0.20)\n",
    "\n",
    "frac_df\n",
    "\n",
    "\n",
    "df = soda_data.head()\n",
    "frac_df = soda_data.tail()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r7kzz9AJNOOQ"
   },
   "source": [
    "**Create a linear regression model**\n",
    "\n",
    "Use scikit-learn to fit a linear regression model to your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eulLo_ZnQA1d"
   },
   "outputs": [],
   "source": [
    "from posixpath import split\n",
    "# Your code goes here\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "tempframe = pd.DataFrame({'Temp':frac_df['T_degC'].values, 'salt':frac_df['Salnty'].values})\n",
    "\n",
    "tempframe = pd.DataFrame({'Temp':frac_df['T_degC'].values,'salt':frac_df['Salnty'].values})\n",
    "tempframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5lye2Pf_QUEA"
   },
   "source": [
    "**Test your model**\n",
    "\n",
    "Use your test data to make predictions and then find the mean squared error of those predictions vs. the actual measured temperatures for the test data.\n",
    "(scikit-learn has functionality to calculate the mean squared error.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RagUyVyvQBO9"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "salt_mass = lin_reg.predict(tempframe)['salt'].values.reshape(-1,1))\n",
    "\n",
    "mean_squared_error(tempframe['Temp'].values.reshape(-1,1), salt_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mgStCm0HTYT_"
   },
   "source": [
    "**Plot your regression line**\n",
    "\n",
    "Create another plot that contains the scatterplot of the salinity and temperatures. Draw the prediction line over the scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YAStL90PQBj6"
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Su3zVHG2ffZw"
   },
   "source": [
    "**Dig deeper**\n",
    "\n",
    "The model we built wasn't very good, but we only used one feature. Are there other features or combinations of features that are more predictive of temperature?\n",
    "\n",
    "Measurements were recorded at different depths. Is salinity a good predictor of temperature at any depth range?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tK1898HdHTdh"
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_9anS4V6RRb5"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "copyright",
    "lY0RT_8fGgD1",
    "Nwo_ivq3IuZB",
    "K-aV3H3vJcjm",
    "exercise-1-key-1",
    "exercise-2-key-1",
    "exercise-3-key-1",
    "exercise-4-key-1",
    "exercise-5-key-1",
    "exercise-6-key-1",
    "exercise-7-key-1",
    "exercise-8-key-1",
    "exercise-9-key-1",
    "exercise-10-key-1"
   ],
   "include_colab_link": true,
   "name": "Linear Regression With scikit-learn",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2ea91e475e4315408580f6d413d12053324791309ae3833caabbce328f0f155b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
